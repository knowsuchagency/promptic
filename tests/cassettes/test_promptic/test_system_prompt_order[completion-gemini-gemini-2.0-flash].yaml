interactions:
- request:
    body: '{"contents": [{"role": "user", "parts": [{"text": "Chat: Hello"}]}], "system_instruction":
      {"parts": [{"text": "You are a helpful test assistant"}, {"text": "You are a
      helpful test assistant"}]}, "generationConfig": {"temperature": 0}}'
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '235'
      content-type:
      - application/json
      host:
      - generativelanguage.googleapis.com
      user-agent:
      - litellm/1.56.3
    method: POST
    uri: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=REDACTED
  response:
    body:
      string: "{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\":
        [\n          {\n            \"text\": \"Hello! How can I help you today?\\n\"\n
        \         }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\":
        \"STOP\",\n      \"avgLogprobs\": -0.003483308106660843\n    }\n  ],\n  \"usageMetadata\":
        {\n    \"promptTokenCount\": 15,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\":
        25,\n    \"promptTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n
        \       \"tokenCount\": 15\n      }\n    ],\n    \"candidatesTokensDetails\":
        [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 10\n
        \     }\n    ]\n  },\n  \"modelVersion\": \"gemini-2.0-flash\",\n  \"responseId\":
        \"fQlmaI-7Mu26x_AP6tyD6Ag\"\n}\n"
    headers:
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
      Content-Type:
      - application/json; charset=UTF-8
      Date:
      - Thu, 03 Jul 2025 04:39:26 GMT
      Server:
      - scaffolding on HTTPServer2
      Server-Timing:
      - gfet4t7; dur=520
      Transfer-Encoding:
      - chunked
      Vary:
      - Origin
      - X-Origin
      - Referer
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - SAMEORIGIN
      X-XSS-Protection:
      - '0'
      content-length:
      - '681'
    status:
      code: 200
      message: OK
- request:
    body: '{"contents": [{"role": "user", "parts": [{"text": "Chat: Hello"}]}, {"role":
      "model", "parts": [{"text": "Hello! How can I help you today?\n"}]}, {"role":
      "user", "parts": [{"text": "Chat: How are you?"}]}], "system_instruction": {"parts":
      [{"text": "You are a helpful test assistant"}, {"text": "You are a helpful test
      assistant"}]}, "generationConfig": {"temperature": 0}}'
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '374'
      content-type:
      - application/json
      host:
      - generativelanguage.googleapis.com
      user-agent:
      - litellm/1.56.3
    method: POST
    uri: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=REDACTED
  response:
    body:
      string: "{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\":
        [\n          {\n            \"text\": \"As a large language model, I don't
        experience feelings in the same way humans do. But I'm functioning well and
        ready to assist you with your requests. Thanks for asking! How are you doing
        today?\\n\"\n          }\n        ],\n        \"role\": \"model\"\n      },\n
        \     \"finishReason\": \"STOP\",\n      \"avgLogprobs\": -0.11626747979058159\n
        \   }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 31,\n    \"candidatesTokenCount\":
        45,\n    \"totalTokenCount\": 76,\n    \"promptTokensDetails\": [\n      {\n
        \       \"modality\": \"TEXT\",\n        \"tokenCount\": 31\n      }\n    ],\n
        \   \"candidatesTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n
        \       \"tokenCount\": 45\n      }\n    ]\n  },\n  \"modelVersion\": \"gemini-2.0-flash\",\n
        \ \"responseId\": \"fglmaPXdIuGg1PIP1dfd6Ao\"\n}\n"
    headers:
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
      Content-Type:
      - application/json; charset=UTF-8
      Date:
      - Thu, 03 Jul 2025 04:39:27 GMT
      Server:
      - scaffolding on HTTPServer2
      Server-Timing:
      - gfet4t7; dur=679
      Transfer-Encoding:
      - chunked
      Vary:
      - Origin
      - X-Origin
      - Referer
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - SAMEORIGIN
      X-XSS-Protection:
      - '0'
      content-length:
      - '842'
    status:
      code: 200
      message: OK
- request:
    body: '{"contents": [{"role": "user", "parts": [{"text": "Chat: Hello"}]}], "system_instruction":
      {"parts": [{"text": "You are a helpful assistant"}, {"text": "You always provide
      concise answers"}, {"text": "You are a helpful assistant"}, {"text": "You always
      provide concise answers"}]}, "generationConfig": {"temperature": 0}}'
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '321'
      content-type:
      - application/json
      host:
      - generativelanguage.googleapis.com
      user-agent:
      - litellm/1.56.3
    method: POST
    uri: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=REDACTED
  response:
    body:
      string: "{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\":
        [\n          {\n            \"text\": \"Hello!\\n\"\n          }\n        ],\n
        \       \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n
        \     \"avgLogprobs\": -0.0025382287179430327\n    }\n  ],\n  \"usageMetadata\":
        {\n    \"promptTokenCount\": 23,\n    \"candidatesTokenCount\": 3,\n    \"totalTokenCount\":
        26,\n    \"promptTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n
        \       \"tokenCount\": 23\n      }\n    ],\n    \"candidatesTokensDetails\":
        [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 3\n      }\n
        \   ]\n  },\n  \"modelVersion\": \"gemini-2.0-flash\",\n  \"responseId\":
        \"fwlmaPqAHezi1PIPzajVyQc\"\n}\n"
    headers:
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
      Content-Type:
      - application/json; charset=UTF-8
      Date:
      - Thu, 03 Jul 2025 04:39:27 GMT
      Server:
      - scaffolding on HTTPServer2
      Server-Timing:
      - gfet4t7; dur=473
      Transfer-Encoding:
      - chunked
      Vary:
      - Origin
      - X-Origin
      - Referer
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - SAMEORIGIN
      X-XSS-Protection:
      - '0'
      content-length:
      - '654'
    status:
      code: 200
      message: OK
version: 1
